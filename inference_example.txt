texts = [
    "This policy mentions data collection and user rights.",
    "We share your information with third-party advertisers.",
]

# Tokenize batch
inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=512)

# Get predictions
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    predictions = logits.argmax(axis=-1).tolist()

# Map predictions to labels
predicted_labels = [label_map[pred] for pred in predictions]
print(predicted_labels)
